{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section II: LSTM\n",
    "------------\n",
    "- In this section, we build a training model comprising a deep LSTM network concatenated with a multiple hidden layers fully-connected network.\n",
    "- The output of our training model is a logit. Then we compute its corresponding probability using sigmoid function. After that we can calculate the cross entropy loss by comparing with the true labels.\n",
    "- The optimizer we are using is AdamOptimizer, which is much faster than the SGD or batch GD. Each time we input a batch of size 256, do the optimization then check its loss. Keep running it untill we hit the loss we want. \n",
    "- Because the computation of cross validation is too expensive for my laptop, we didn't do it. To prevent overfitting, what we do is: \n",
    "    1. run several optimization steps \n",
    "    2. check the loss of current batch\n",
    "    3. use the model to process the test set\n",
    "    4. upload the result to Kaggle to see how does it perform\n",
    "- Note that to run this LSTM statically, first we find the maximal sentence length between the train set and test set, mark it as our timestep in one layer LSTM cell. Then we pad all the sentence with zero vectors to ensure they all have the same length. \n",
    "------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hiroyukiqaq/.virtualenvs/cv/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the vectorized twitter dataset we got from Section I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define the file reading function. \n",
    "## Return the train data, test data and the labels data respectively.\n",
    "\n",
    "def import_matrix(f_name):\n",
    "    with np.load(f_name) as data:\n",
    "        train_matrix = data['train_matrix']\n",
    "        test_matrix = data['test_matrix']\n",
    "        train_labels = data['train_labels']\n",
    "    return train_matrix, test_matrix, train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_matrix, test_matrix, train_labels = import_matrix('embedding_matrix.npz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the maximal sentence length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define a function to find the maximal length.\n",
    "## The input is the train set or test set, return their maximal length respectively.\n",
    "\n",
    "def max_length(matrix):\n",
    "    matrix_iter = matrix.__iter__()\n",
    "    max_length = 0\n",
    "    for i in range(len(matrix)):\n",
    "        temp = len(matrix_iter.__next__())\n",
    "        if temp > max_length:\n",
    "            max_length = temp\n",
    "    return max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max sentence length in both train.csv and test.csv is  41\n"
     ]
    }
   ],
   "source": [
    "max_size = max(max_length(train_matrix), max_length(test_matrix))\n",
    "print('max sentence length in both train.csv and test.csv is ', max_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pad all the twitters with zero vectors to make them have same length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Define the zero_padding function.\n",
    "## Input the maximal length we just find and the dataset, it will do the job.\n",
    "\n",
    "def zero_padding(matrix, max_size):\n",
    "    pad = np.zeros(300)\n",
    "    for i in range(len(matrix)):\n",
    "        if len(matrix[i]) < max_size:\n",
    "            temp = len(matrix[i])\n",
    "            diff = max_size - temp\n",
    "            for j in range(diff):\n",
    "                matrix[i].append(pad) \n",
    "        matrix[i] = np.asarray(matrix[i])\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_matrix = zero_padding(train_matrix, max_size)\n",
    "test_matrix = zero_padding(test_matrix, max_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert out data to the standard numpy ndarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For the convenience of manipulation, we convert our data set to numpy ndarray.\n",
    "\n",
    "def ndarray_convert(matrix, max_size):\n",
    "    temp = np.zeros((len(matrix), max_size, 300))\n",
    "    for i in range(len(temp)):\n",
    "        temp[i] = matrix[i]\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the shape of train set is  (5000, 41, 300)\n",
      "the shape of test set is  (1444, 41, 300)\n"
     ]
    }
   ],
   "source": [
    "## The shape of our dataset is just what we expected.\n",
    "\n",
    "train_matrix = ndarray_convert(train_matrix, max_size)\n",
    "print('the shape of train set is ',train_matrix.shape)\n",
    "test_matrix = ndarray_convert(test_matrix, max_size)\n",
    "print('the shape of test set is ',test_matrix.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## To use the TensorFlow build-in batching interface, \n",
    "## we convert our training data along with its labels to Tensors.\n",
    "## The Tensors are not iterable, so we need to slice them into \n",
    "## sliced Dataset, then batching them.\n",
    "\n",
    "## Convert to Tensors.\n",
    "train_tensor = tf.convert_to_tensor(train_matrix, dtype=tf.float32)\n",
    "labels_tensor = tf.convert_to_tensor(train_labels, dtype=tf.float32)\n",
    "\n",
    "\n",
    "## Convert to sliced dataset.\n",
    "train_slices = tf.data.Dataset.from_tensor_slices(train_tensor)\n",
    "labels_slices = tf.data.Dataset.from_tensor_slices(labels_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Batching the dataset with batch size 256.\n",
    "## Note that the size of the dataset may not be perfectly divided by the batch size.\n",
    "## Thus the last batch of the dataset have the size of remainder.\n",
    "## When hitting the bottom, the iterator will start from beginning again.\n",
    "batch_size = 256\n",
    "train_batch = train_slices.batch(batch_size).repeat()\n",
    "labels_batch = labels_slices.batch(batch_size).repeat()\n",
    "\n",
    "\n",
    "## Define the iterator of our batched dataset.\n",
    "train_iter = train_batch.make_initializable_iterator()\n",
    "labels_iter = labels_batch.make_initializable_iterator()\n",
    "\n",
    "next_train = train_iter.get_next()\n",
    "next_label = labels_iter.get_next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Model building\n",
    "------------\n",
    "+ This is where we build our model. The model has two parts:\n",
    "    + LSTM: it's a three layers LSTM network which the number of cell units is the same of the maximal length of the sentence, with initial state zeros and tanh activation function. The input of next layer is the output of last layer. The input of the first layer is a sentence vector with shape (41, 300), which means it have 41 words and each word is 300 dimension vector. The output of it is the last time step output of the third layer.\n",
    "    + Fully-connected NN: it also has 3 hidden layers, with number of neurons 20, 30 and 10 respectively. The input of first layer is the output of our LSTM module. The activation function of the 3 hidden layers are leaky RELU, and the output layer has no activation function. It's just the simple scalar of the dot product result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Construct two placeholders for the input data and labels.\n",
    "\n",
    "inputs = tf.placeholder(tf.float32, shape=(None, 41, 300))\n",
    "labels = tf.placeholder(tf.float32, shape=(None, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Build the 3 LSTM cells.\n",
    "\n",
    "with tf.variable_scope(\"lstm1\"):\n",
    "    lstm_cell1 = tf.contrib.rnn.BasicLSTMCell(41, forget_bias=1.0, activation=tf.tanh) \n",
    "with tf.variable_scope(\"lstm2\"):\n",
    "    lstm_cell2 = tf.contrib.rnn.BasicLSTMCell(41, forget_bias=1.0, activation=tf.tanh) \n",
    "with tf.variable_scope(\"lstm3\"):\n",
    "    lstm_cell3 = tf.contrib.rnn.BasicLSTMCell(41, forget_bias=1.0, activation=tf.tanh) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Bind the three LSTM cell together to become a 3 layers deep RNN model.\n",
    "\n",
    "with tf.variable_scope(\"lstm1\"):\n",
    "    outputs1, state1 = tf.contrib.rnn.static_rnn(lstm_cell1, tf.unstack(inputs, axis=1), dtype=tf.float32)\n",
    "with tf.variable_scope(\"lstm2\"):\n",
    "    outputs2, state2 = tf.contrib.rnn.static_rnn(lstm_cell2, outputs1, dtype=tf.float32)\n",
    "with tf.variable_scope(\"lstm3\"):\n",
    "    outputs3, state3 = tf.contrib.rnn.static_rnn(lstm_cell3, outputs2, dtype=tf.float32)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Build the 3 hidden layers fully-connected neuron network.\n",
    "\n",
    "with tf.variable_scope(\"hidden1\"):\n",
    "    hidden_output1 = tf.contrib.layers.fully_connected(outputs3[-1], 20, activation_fn=tf.nn.leaky_relu)\n",
    "with tf.variable_scope(\"hidden2\"):\n",
    "    hidden_output2 = tf.contrib.layers.fully_connected(hidden_output1, 30, activation_fn=tf.nn.leaky_relu)\n",
    "with tf.variable_scope(\"hidden3\"):\n",
    "    hidden_output3 = tf.contrib.layers.fully_connected(hidden_output2, 10, activation_fn=tf.nn.leaky_relu)\n",
    "with tf.variable_scope(\"output\"):   \n",
    "    logits = tf.contrib.layers.fully_connected(hidden_output3, 1, activation_fn=None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the cross entropy loss function.\n",
    "loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=labels, logits=logits)\n",
    "\n",
    "# Initialize the AdamOptimizer\n",
    "adamoptimizer = tf.train.AdamOptimizer()\n",
    "train_op = adamoptimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The graph below is visualization of my whole workflow.\n",
    "![title](graph.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, None]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Define the session and initialize all the variables and iterators. \n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "sess.run([train_iter.initializer, labels_iter.initializer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the log loss of current batch is  0.11498016\n",
      "the log loss of current batch is  0.19568348\n",
      "the log loss of current batch is  0.1094781\n",
      "the log loss of current batch is  0.14224565\n",
      "the log loss of current batch is  0.15021595\n",
      "the log loss of current batch is  0.11847182\n",
      "the log loss of current batch is  0.087889284\n",
      "the log loss of current batch is  0.15502524\n",
      "the log loss of current batch is  0.12887405\n",
      "the log loss of current batch is  0.071093015\n"
     ]
    }
   ],
   "source": [
    "## Here is where we do the 'step by step' run and check trick to prevent overfitting.\n",
    "## After each loop we will use the current model to predict the test set then upload \n",
    "## to Kaggle to check the result.\n",
    "\n",
    "for step in range(10):\n",
    "    temp_input = sess.run(next_train)\n",
    "    temp_label = sess.run(next_label)\n",
    "    _, temp_loss = sess.run([train_op, loss], feed_dict={inputs: temp_input, labels: temp_label.reshape([-1, 1])})\n",
    "    current_loss = temp_loss\n",
    "    print('the log loss of current batch is ', np.mean(current_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run the model using the test dataset as input to get the corresponding logits.\n",
    "\n",
    "result = sess.run(logits, feed_dict={inputs: test_matrix})\n",
    "result = result.reshape(1444)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define a sigmoid function to convert the logits to probabilities.\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x, dtype=np.float64))\n",
    "\n",
    "\n",
    "## Output the result to a csv file according to the required format.\n",
    "## Note that the result is probabilities of being '1', or 'realDonaldTrump'.\n",
    "## To get the probabilities of being '0', or 'HillaryClinton', we only need to \n",
    "## compute 1-result.\n",
    "def out_csv(test_result, f_name):\n",
    "\n",
    "    temp = np.zeros([3, 1444])\n",
    "    tweet_id = np.arange(1444, dtype=np.int)\n",
    "    prob_trump = test_result\n",
    "    prob_hillary = 1 - test_result\n",
    "    temp[0] = tweet_id\n",
    "    # temp[0] = temp[0].astype(int)\n",
    "    temp[1] = prob_trump\n",
    "    temp[2] = prob_hillary\n",
    "    np.savetxt(f_name, temp.T, header=\"id,realDonaldTrump,HillaryClinton\", comments=\"\", fmt=\"%i,%.18e,%.18e\")\n",
    "    return 0    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_csv(sigmoid(result), 'result.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
